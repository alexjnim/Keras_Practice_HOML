{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transfer learning for sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"keras_models/02_sequential_API_regressor.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that model_A and model_B_on_A now share some layers. When you train model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone model_A before you reuse its layers. To do this, you clone model A’s architecture with clone_model(), then copy its weights (since clone_model() does not clone the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you could train model_B_on_A for task B, but since the new output layer was initialized randomly it will make large errors (at least during the first few epochs), so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, set every layer’s trainable attribute to False and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: You must always compile your model after you freeze or unfreeze layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid damaging the reused weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/4\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: -15.8214 - accuracy: 0.0028 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 2/4\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 3/4\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 4/4\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/16\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 2/16\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 3/16\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 4/16\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 5/16\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 6/16\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 7/16\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 8/16\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 9/16\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 10/16\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 11/16\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 12/16\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 13/16\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 14/16\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 15/16\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n",
      "Epoch 16/16\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: -16.2996 - accuracy: 0.0029 - val_loss: -15.9821 - val_accuracy: 7.7519e-04\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train, y_train, epochs=4,\n",
    "                           validation_data=(X_valid, y_valid))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "    \n",
    "optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_on_A.fit(X_train, y_train, epochs=16,\n",
    "                           validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 15us/sample - loss: -16.5181 - accuracy: 0.0048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-16.518108403405478, 0.004844961]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terrible results!! You have to try many configurations until you find one that demonstrates a strong improvement. If you try to change the classes or the random seed, you will see that the improvement generally drops, or even vanishes or reverses. You have to keep “torturing the data until it confesses.”\n",
    "\n",
    "It turns out that transfer learning does not work very well with small dense networks, presumably because small networks learn few patterns, and dense networks learn very specific patterns, which are unlikely to be useful in other tasks. Transfer learning works best with deep convolutional neural networks, which tend to learn feature detectors that are much more general (especially in the lower layers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
